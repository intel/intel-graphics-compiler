/*========================== begin_copyright_notice ============================

Copyright (C) 2017-2021 Intel Corporation

SPDX-License-Identifier: MIT

============================= end_copyright_notice ===========================*/

//===----------------------------------------------------------------------===//
//
/// CMImpParam
/// ----------
///
/// As well as explicit kernel args declared in the CM kernel function, certain
/// implicit args are also passed. These fall into 3 categories:
///
/// 1. fields set up in r0 by the hardware, depending on which dispatch method
///    is being used (e.g. media walker);
///
/// 2. implicit args set up along with the explicit args in CURBE by the CM
///    runtime.
///
/// 3. implicit OCL/L0 args set up, e.g. private base, byval arg linearization.
///
/// The r0 implicit args are represented in LLVM IR by special intrinsics, and the
/// GenX backend generates these to special reserved vISA registers.
///
/// For the CM runtime implicit args in (2) above, in vISA 3.2 and earlier, these were also
/// represented by LLVM special intrinsics and vISA special reserved vISA registers.
/// Because they are specific to the CM runtime, and not any other user of vISA,
/// vISA 3.3 has removed them, and instead they are handled much like other kernel
/// args in the input table.
///
/// The *kind* byte in the input table has two fields:
///
/// * the *category* field, saying whether the input is general/surface/etc;
///
/// * the *provenance* field, saying whether the input is an explicit one from
///   the CM source, or an implicit one generated by this pass. This is a
///   protocol agreed between the CM compiler (in fact this pass) and the CM
///   runtime.
///
/// Within the CM compiler, the vISA input table for a kernel is represented by an
/// array of kind bytes, each one corresponding to an argument of the kernel function.
///
/// Clang codegen still generates special intrinsics for these CM runtime implicit
/// args. It is the job of this CMImpParam pass to transform those intrinsics:
///
/// * where the intrinsic for a CM runtime implicit arg is used somewhere:
///
///   - a global variable is created for it;
///
///   - for any kernel that uses the implicit arg (or can reach a subroutine that
///     uses it), the implicit arg is added to the input table in the kernel
///     metadata and as an extra arg to the definition of the kernel itself,
///     and its value is stored into the global variable;
///
/// * each use of the intrinsic for a CM runtime implicit arg is transformed into
///   a load of the corresponding global variable.
///
/// Like any other global variable, the subsequent CMABI pass turns the global
/// variable for an implicit arg into local variable(s) passed into subroutines
/// if necessary.
///
/// This pass also linearizes kernel byval arguments.
/// If a kernel has an input pointer argument with byval attribute, it means
/// that it will be passed as a value with the argument's size = sizeof(the
/// type), not sizeof(the type *). To support such kinds of arguments, VC (as
/// well as scalar IGC) makes implicit linearization, e.g.
///
///   %struct.s1 = type { [2 x i32], i8 } ===> i32, i32, i8
///
/// This implicit linearization is added as kernel arguments and mapped via
/// metadata to the original explicit byval argument.
///
///   %struct.s1 = type { [2 x i32], i8 }
///
///   declare i32 @foo(%struct.s1* byval(%struct.s1) "VCArgumentDesc"="svmptr_t"
///                    "VCArgumentIOKind"="0" "VCArgumentKind"="0" %arg, i64
///                    %arg1);
///
/// Will be transformed into (byval args uses will be changed in
/// CMKernelArgOffset)
///
///   declare i32 @foo(%struct.s1* byval(%struct.s1) "VCArgumentDesc"="svmptr_t"
///                     "VCArgumentIOKind"="0" "VCArgumentKind"="0" %arg, i64
///                     %arg1, i32 %__arg_lin__arg_0, i32 %__arg_lin__arg_1, i8
///                     %__arg_lin__arg_2);
///
/// Additionally, information about these implicit linearization will be written
/// to kernel metadata as internal::KernelMDOp::LinearizationArgs. It stores
/// mapping between explicit byval argument and its linearization.
///
//===----------------------------------------------------------------------===//

#define DEBUG_TYPE "cmimpparam"

#include "vc/GenXOpts/GenXOpts.h"
#include "vc/GenXOpts/Utils/KernelInfo.h"

#include "vc/Utils/General/FunctionAttrs.h"
#include "vc/Utils/General/DebugInfo.h"

#include "llvm/ADT/SCCIterator.h"
#include "llvm/ADT/STLExtras.h"
#include "llvm/ADT/SetVector.h"
#include "llvm/ADT/Statistic.h"
#include "llvm/Analysis/CallGraph.h"
#include "llvm/Analysis/CallGraphSCCPass.h"
#include "llvm/GenXIntrinsics/GenXIntrinsics.h"
#include "llvm/IR/CFG.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/InstIterator.h"
#include "llvm/IR/Intrinsics.h"
#include "llvm/IR/Module.h"
#include "llvm/InitializePasses.h"
#include "llvm/Pass.h"
#include "llvm/Support/CommandLine.h"
#include "llvm/Support/Debug.h"
#include "llvm/Support/raw_ostream.h"

#include <map>
#include <stack>
#include <unordered_map>
#include <unordered_set>
#include <vector>

#include "Probe/Assertion.h"
#include "llvmWrapper/IR/DerivedTypes.h"
#include "llvmWrapper/IR/Function.h"

using namespace llvm;

static cl::opt<bool>
    CMRTOpt("cmimpparam-cmrt", cl::init(true), cl::Hidden,
            cl::desc("Should be used only in llvm opt to switch RT"));

namespace {

// Helper struct to store temporary information for implicit arguments
// linearization.
struct LinearizationElt {
  Type *Ty;
  unsigned Offset;
};
using LinearizedTy = std::vector<LinearizationElt>;
using ArgLinearization = std::unordered_map<Argument *, LinearizedTy>;
using ImplArgIntrSeq = std::vector<CallInst *>;
using IntrIDSet = std::unordered_set<unsigned>;
using IntrIDMap = std::unordered_map<Function *, IntrIDSet>;

// Implicit args in this pass are denoted by the corresponding intrinsic ID.
// But not all implicit args have a corresponding intrinsic. So for those args
// pseudo intrinsic IDs are provided. Pseudo ID values are guaranteed to not
// overlap with real instrinsic IDs.
namespace PseudoIntrinsic {
enum Enum : unsigned {
  First = GenXIntrinsic::not_any_intrinsic,
  PrivateBase = First,
  Last
};
} // namespace PseudoIntrinsic

struct CMImpParam : public ModulePass {
  static char ID;
  bool IsCmRT;
  const DataLayout *DL = nullptr;

  CMImpParam(bool isCmRT = true) : ModulePass(ID), IsCmRT(isCmRT && CMRTOpt) {
    initializeCMImpParamPass(*PassRegistry::getPassRegistry());
  }

  void getAnalysisUsage(AnalysisUsage &AU) const override {
    AU.addRequired<CallGraphWrapperPass>();
  }

  StringRef getPassName() const override { return "CM Implicit Params"; }

  bool runOnModule(Module &M) override;

private:
  void replaceWithGlobal(CallInst *CI);
  void replaceImplicitArgIntrinsics(const ImplArgIntrSeq &Workload);
  void WriteArgsLinearizationInfo(Module &M);

  LinearizedTy LinearizeAggregateType(Type *AggrTy);
  ArgLinearization GenerateArgsLinearizationInfo(Function &F);

  CallGraphNode *processKernel(Function *F, const IntrIDSet &UsedImplicits);

  static Value *getValue(Metadata *M) {
    if (auto VM = dyn_cast<ValueAsMetadata>(M))
      return VM->getValue();
    return nullptr;
  }

  // Convert to implicit thread payload related intrinsics.
  void ConvertToOCLPayload(Module &M);

  uint32_t MapToKind(unsigned IID) {
    using namespace genx;
    switch (IID) {
      default:
        return KernelMetadata::AK_NORMAL;
      case GenXIntrinsic::genx_print_buffer:
        return KernelMetadata::AK_NORMAL | KernelMetadata::IMP_OCL_PRINTF_BUFFER;
      case GenXIntrinsic::genx_local_size:
        return KernelMetadata::AK_NORMAL | KernelMetadata::IMP_LOCAL_SIZE;
      case GenXIntrinsic::genx_local_id:
      case GenXIntrinsic::genx_local_id16:
        return KernelMetadata::AK_NORMAL | KernelMetadata::IMP_LOCAL_ID;
      case GenXIntrinsic::genx_group_count:
        return KernelMetadata::AK_NORMAL | KernelMetadata::IMP_GROUP_COUNT;
      case GenXIntrinsic::genx_get_scoreboard_deltas:
        return KernelMetadata::AK_NORMAL | KernelMetadata::IMP_SB_DELTAS;
      case GenXIntrinsic::genx_get_scoreboard_bti:
        return KernelMetadata::AK_SURFACE | KernelMetadata::IMP_SB_BTI;
      case GenXIntrinsic::genx_get_scoreboard_depcnt:
        return KernelMetadata::AK_SURFACE | KernelMetadata::IMP_SB_DEPCNT;
      case PseudoIntrinsic::PrivateBase:
        return KernelMetadata::AK_NORMAL | KernelMetadata::IMP_OCL_PRIVATE_BASE;
    }
    return KernelMetadata::AK_NORMAL;
  }

  GlobalVariable *getOrCreateGlobalForIID(Function *F, unsigned IID) {
    if (GlobalsMap.count(IID))
      return GlobalsMap[IID];

    Type * Ty = getIntrinRetType(F->getContext(), IID);
    IGC_ASSERT(Ty);

    auto IntrinsicName = GenXIntrinsic::getAnyName(IID, None);
    GlobalVariable *NewVar = new GlobalVariable(
        *F->getParent(), Ty, false, GlobalVariable::InternalLinkage,
        UndefValue::get(Ty), "__imparg_" + IntrinsicName);
    GlobalsMap[IID] = NewVar;

    addDebugInfoForImplicitGlobal(*NewVar, IntrinsicName);

    return NewVar;
  }

  static void addDebugInfoForImplicitGlobal(GlobalVariable &Var,
                                            StringRef Name) {
    auto &M = *Var.getParent();
    if (!vc::DIBuilder::checkIfModuleHasDebugInfo(M))
      return;

    std::string DiName = (Twine("__") + Name).str();
    std::replace(DiName.begin(), DiName.end(), '.', '_');

    vc::DIBuilder DBuilder(M);
    auto *DGVType = DBuilder.translateTypeToDIType(*Var.getValueType());
    if (!DGVType) {
      LLVM_DEBUG(dbgs() << "ERROR: could not create debug info for implict var:"
                 << Var << "\n");
      return;
    }
    auto *GVE = DBuilder.createGlobalVariableExpression(DiName, DiName, DGVType);
    Var.addDebugInfo(GVE);
  }

  static Type *getIntrinRetType(LLVMContext &Context, unsigned IID) {
    switch (IID) {
      case GenXIntrinsic::genx_print_buffer:
      case PseudoIntrinsic::PrivateBase:
        return llvm::Type::getInt64Ty(Context);
      case GenXIntrinsic::genx_local_id:
      case GenXIntrinsic::genx_local_size:
      case GenXIntrinsic::genx_group_count:
        return IGCLLVM::FixedVectorType::get(llvm::Type::getInt32Ty(Context),
                                             3);
      case GenXIntrinsic::genx_local_id16:
        return IGCLLVM::FixedVectorType::get(llvm::Type::getInt16Ty(Context),
                                             3);
      default:
        // Should be able to extract the type from the intrinsic
        // directly as no overloading is required (if it is then
        // you need to define specific type in a case statement above)
        FunctionType *FTy = dyn_cast_or_null<FunctionType>(
                                    GenXIntrinsic::getAnyType(Context, IID));
        if (FTy)
          return FTy->getReturnType();
    }
    return nullptr;
  }

  // GlobalVariables that have been created for an intrinsic
  SmallDenseMap<unsigned, GlobalVariable *> GlobalsMap;
};

// A helper class to recursively traverse call graph and collect all the
// required implicit args.
// Only temporary objects should be constructed. Usage:
// CallGraphTraverser{CG, UsedIntr}.collectIndirectlyUsedImplArgs(F);
class CallGraphTraverser {
  const CallGraph &CG;
  const IntrIDMap &UsedIntr;
  std::unordered_set<Function *> Visited;
  IntrIDSet CollectedIID;

public:
  CallGraphTraverser(const CallGraph &CGIn, const IntrIDMap &UsedIntrIn)
      : CG{CGIn}, UsedIntr{UsedIntrIn} {}
  IntrIDSet collectIndirectlyUsedImplArgs(Function &F) && {
    visitFunction(F);
    return CollectedIID;
  }

private:
  void visitFunction(Function &F);
};

} // namespace

static ImplArgIntrSeq collectImplicitArgIntrinsics(Module &M);
static IntrIDMap fillUsedIntrMap(const ImplArgIntrSeq &Workload);

static bool isPseudoIntrinsic(unsigned IID) {
  return IID >= PseudoIntrinsic::First && IID < PseudoIntrinsic::Last;
}

// Collect all CM kernels from named metadata.
static std::vector<Function *> collectKernels(Module &M) {
  NamedMDNode *Named = M.getNamedMetadata(genx::FunctionMD::GenXKernels);
  if (!Named)
    return {};
  std::vector<Function *> Kernels;
  for (unsigned I = 0, E = Named->getNumOperands(); I != E; ++I) {
    MDNode *Node = Named->getOperand(I);
    auto *F = cast<Function>(
        cast<ValueAsMetadata>(Node->getOperand(genx::KernelMDOp::FunctionRef))
            ->getValue());
    Kernels.push_back(F);
  }
  return Kernels;
}

bool CMImpParam::runOnModule(Module &M) {
  DL = &M.getDataLayout();

  // Apply necessary changes if kernels are compiled for OpenCL runtime.
  ConvertToOCLPayload(M);

  // Analyze functions for implicit use intrinsic invocation
  ImplArgIntrSeq Workload = collectImplicitArgIntrinsics(M);
  auto Kernels = collectKernels(M);

  if (Workload.empty() && Kernels.empty())
    // If ConvertToOCLPayload changed code, workload wouldn't be empty (there
    // would be at least local_id16 intrinsics). So returning false here is
    // correct.
    return false;

  IntrIDMap UsedIntrInfo = fillUsedIntrMap(Workload);
  replaceImplicitArgIntrinsics(Workload);

  CallGraph &CG = getAnalysis<CallGraphWrapperPass>().getCallGraph();
  // Kernel transformation should go last since it invalidates the collected
  // data: kernel functions are changed.
  for (Function *Kernel : Kernels) {
    // Traverse the call graph to determine what the total implicit uses are for
    // the top level kernels.
    IntrIDSet RequiredImplArgs =
        CallGraphTraverser{CG, UsedIntrInfo}.collectIndirectlyUsedImplArgs(
            *Kernel);
    // For OCL/L0 RT we should unconditionally add implicit PRIVATE_BASE
    // argument which is not supported on CM RT.
    if (!IsCmRT)
      RequiredImplArgs.emplace(PseudoIntrinsic::PrivateBase);
    genx::internal::createInternalMD(*Kernel);
    if (!RequiredImplArgs.empty())
      processKernel(Kernel, RequiredImplArgs);
  }

  return true;
}

// Replace the given instruction with a load from a global
// The method erases the original call instruction.
void CMImpParam::replaceWithGlobal(CallInst *CI) {
  IGC_ASSERT_MESSAGE(GenXIntrinsic::isGenXIntrinsic(CI),
                     "genx intrinsic is expected");
  auto IID = GenXIntrinsic::getGenXIntrinsicID(CI->getCalledFunction());
  GlobalVariable *GV =
      getOrCreateGlobalForIID(CI->getParent()->getParent(), IID);
  LoadInst *Load = new LoadInst(GV->getType()->getPointerElementType(), GV, "",
                                /* isVolatile */ false, CI);
  Load->takeName(CI);
  Load->setDebugLoc(CI->getDebugLoc());
  CI->replaceAllUsesWith(Load);
  CI->eraseFromParent();
}

static bool isSupportedAggregateArgument(Argument &Arg) {
  if (!Arg.getType()->isPointerTy())
    return false;
  if (!Arg.hasByValAttr())
    return false;

  Type *Ty = Arg.getType()->getPointerElementType();
  auto *STy = cast<StructType>(Ty);
  IGC_ASSERT(!STy->isOpaque());
  return true;
}

// A helper structure to store current state of the aggregate traversal.
struct PendingTypeInfo {
  Type *Ty;         // Type to decompose
  unsigned NextElt; // Subelement number to decompose next
  unsigned Offset;  // Offset for the trivial type in Ty
};

// Byval aggregate arguments must be linearized. This function decomposes the
// aggregate type into primitive types recursively.
// Example:
//   struct s1 {
//     struct s2 {
//       int a;
//     };
//     char b;
//   };
//
//                Pending(stack) | LinTy(output)
// Start:
//                s1, 0, 0       | -
// Iteration 0:
//                s1, 1, 4       | -
//                s2, 0, 0       |
//   Comment: two elements in stack. s1, 1, 4 means subtype number 1 in the
//   s1 must be decomposed. The first trivial type in the 1 subtype of s1 will
//   have offset = 4. Note that this subtype may be also an aggregate type. In
//   this case, offset = 4 will be propagated to the first nested trivial type.
//   It is a recursive function, rewritten to use stack, so as not to have
//   recursion problems.
// Iteration 1:
//                s1, 1, 4       | -
//                int,0, 0       | -
// Iteration 2:
//                s1, 1, 4       | int, 0
// Iteration 3:
//                char, 0, 4     | int, 0
// Iteration 4:
//                -              | int, 0
//                               | char, 4
//
LinearizedTy CMImpParam::LinearizeAggregateType(Type *AggrTy) {
  LinearizedTy LinTy;

  std::stack<PendingTypeInfo> Pending;
  Pending.push({AggrTy, 0, 0});

  while (!Pending.empty()) {
    PendingTypeInfo Info = Pending.top();
    Pending.pop();
    Type *CurTy = Info.Ty;
    unsigned CurElt = Info.NextElt;
    unsigned NextElt = CurElt + 1;
    if (auto *STy = dyn_cast<StructType>(CurTy)) {
      unsigned NumElts = STy->getStructNumElements();
      const StructLayout *Layout = DL->getStructLayout(STy);

      IGC_ASSERT(CurElt < NumElts);
      Type *EltType = STy->getElementType(CurElt);
      if (NumElts > NextElt) {
        unsigned CurOffset = Layout->getElementOffset(CurElt);
        unsigned EltOffset = Layout->getElementOffset(NextElt) - CurOffset;
        Pending.push({CurTy, NextElt, Info.Offset + EltOffset});
      }
      Pending.push({EltType, 0, Info.Offset});

    } else if (auto *ATy = dyn_cast<ArrayType>(CurTy)) {
      unsigned NumElts = ATy->getNumElements();
      Type *EltTy = CurTy->getContainedType(0);
      unsigned EltSize = DL->getTypeStoreSize(EltTy);

      if (NumElts > NextElt)
        Pending.push({Info.Ty, NextElt, Info.Offset + EltSize});
      Pending.push({EltTy, 0, Info.Offset});
    } else
      LinTy.push_back({CurTy, Info.Offset});
  }

  return LinTy;
}

// For each byval aggregate calculate types of implicit args and their offsets
// in this aggregate.
ArgLinearization CMImpParam::GenerateArgsLinearizationInfo(Function &F) {
  ArgLinearization Lin;
  for (auto &Arg : F.args()) {
    if (!isSupportedAggregateArgument(Arg))
      continue;

    Type *ArgTy = Arg.getType();
    IGC_ASSERT(isa<PointerType>(ArgTy));
    auto *STy = cast<StructType>(ArgTy->getPointerElementType());
    Lin[&Arg] = LinearizeAggregateType(STy);
  }
  return Lin;
}

static bool isImplicitArgIntrinsic(const Function &F) {
  auto IID = GenXIntrinsic::getGenXIntrinsicID(&F);
  switch (IID) {
  case GenXIntrinsic::genx_local_size:
  case GenXIntrinsic::genx_local_id:
  case GenXIntrinsic::genx_local_id16:
  case GenXIntrinsic::genx_group_count:
  case GenXIntrinsic::genx_get_scoreboard_deltas:
  case GenXIntrinsic::genx_get_scoreboard_bti:
  case GenXIntrinsic::genx_get_scoreboard_depcnt:
  case GenXIntrinsic::genx_print_buffer:
    return true;
  default:
    return false;
  }
}

// For each function, see if it uses an intrinsic that in turn requires an
// implicit kernel argument
// (such as llvm.genx.local.size)
static ImplArgIntrSeq collectImplicitArgIntrinsics(Module &M) {
  ImplArgIntrSeq Workload;
  auto &&ImplArgIntrinsics = make_filter_range(
      M, [](const Function &F) { return isImplicitArgIntrinsic(F); });
  for (Function &Intr : ImplArgIntrinsics)
    llvm::transform(Intr.users(), std::back_inserter(Workload),
                    [](User *U) { return cast<CallInst>(U); });
  return Workload;
}

static IntrIDMap fillUsedIntrMap(const ImplArgIntrSeq &Workload) {
  IntrIDMap UsedIntrInfo;
  for (CallInst *CI : Workload) {
    auto IID = GenXIntrinsic::getGenXIntrinsicID(CI->getCalledFunction());
    UsedIntrInfo[CI->getFunction()].insert(IID);
  }
  return UsedIntrInfo;
}

// Replace implicit arg intrinsics collected in \p Workload with a load of
// the corresponding __imparg global variable.
// Fill implicit args usage data.
void CMImpParam::replaceImplicitArgIntrinsics(const ImplArgIntrSeq &Workload) {
  for (CallInst *Intr : Workload)
    replaceWithGlobal(Intr);
}

// Convert to implicit thread payload related intrinsics.
void CMImpParam::ConvertToOCLPayload(Module &M) {
  // Check if this kernel is compiled for OpenCL runtime.
  bool DoConversion = false;

  if (NamedMDNode *Named = M.getNamedMetadata(genx::FunctionMD::GenXKernels)) {
    for (unsigned I = 0, E = Named->getNumOperands(); I != E; ++I) {
      MDNode *Node = Named->getOperand(I);
      auto F = dyn_cast_or_null<Function>(
          getValue(Node->getOperand(genx::KernelMDOp::FunctionRef)));
      if (F && (F->hasFnAttribute(genx::FunctionMD::OCLRuntime) || !IsCmRT)) {
        DoConversion = true;
        break;
      }
    }
  }

  if (!DoConversion)
    return;

  auto getFn = [=, &M](unsigned ID, Type *Ty) {
    return M.getFunction(GenXIntrinsic::getAnyName(ID, Ty));
  };

  // Convert genx_local_id -> zext(genx_local_id16)
  Type *Ty32 =
      IGCLLVM::FixedVectorType::get(Type::getInt32Ty(M.getContext()), 3);
  Type *Ty16 =
      IGCLLVM::FixedVectorType::get(Type::getInt16Ty(M.getContext()), 3);
  if (auto LIDFn = getFn(GenXIntrinsic::genx_local_id, Ty32)) {
    Function *LID16 = GenXIntrinsic::getGenXDeclaration(
        &M, GenXIntrinsic::genx_local_id16, Ty16);
    for (auto UI = LIDFn->user_begin(); UI != LIDFn->user_end();) {
      auto UInst = dyn_cast<Instruction>(*UI++);
      if (UInst) {
        IRBuilder<> Builder(UInst);
        Value *Val = Builder.CreateCall(LID16, None, UInst->getName() + ".i16");
        Val = Builder.CreateZExt(Val, Ty32);
        Val->takeName(UInst);
        UInst->replaceAllUsesWith(Val);
        UInst->eraseFromParent();
      }
    }
  }
}

// Determine if the named function uses any functions tagged with implicit use
// in the call-graph
void CallGraphTraverser::visitFunction(Function &F) {
  // If this node has already been processed then return immediately
  if (Visited.count(&F))
    return;

  // Add this node to the already visited list
  Visited.insert(&F);

  // Handle current node: add its used implicit intrinisic IDs if present.
  if (UsedIntr.count(&F))
    CollectedIID.insert(UsedIntr.at(&F).begin(), UsedIntr.at(&F).end());

  // Start the traversal
  const CallGraphNode *N = CG[&F];
  // Inspect all children (recursive)
  for (auto CallEdge : *N) {
    // Returns nullptr for externally defined functions or in case of indirect
    // call.
    auto *Child = CallEdge.second->getFunction();
    if (Child)
      visitFunction(*Child);
  }
}

static std::string getImplicitArgName(unsigned IID) {
  if (!isPseudoIntrinsic(IID))
    return "__arg_" + GenXIntrinsic::getAnyName(IID, None);
  IGC_ASSERT_MESSAGE(IID == PseudoIntrinsic::PrivateBase,
                     "there's only private base pseudo intrinsic for now");
  return "privBase";
}

// Process a kernel - loads from a global (and the globals) have already been
// added if required elsewhere (in doInitialization)
// We've already determined that this is a kernel and that it requires some
// implicit arguments adding
CallGraphNode *CMImpParam::processKernel(Function *F,
                                         const IntrIDSet &UsedImplicits) {
  LLVMContext &Context = F->getContext();

  IGC_ASSERT_MESSAGE(genx::isKernel(F),
                     "ProcessKernel invoked on non-kernel CallGraphNode");

  AttributeList AttrVec;
  const AttributeList &PAL = F->getAttributes();

  ArgLinearization ArgsLin = GenerateArgsLinearizationInfo(*F);

  // Determine the new argument list
  SmallVector<Type *, 8> ArgTys;

  // First transfer all the explicit arguments from the old kernel
  unsigned ArgIndex = 0;
  for (Function::arg_iterator I = F->arg_begin(), E = F->arg_end(); I != E;
       ++I, ++ArgIndex) {
    ArgTys.push_back(I->getType());
    AttributeSet attrs = PAL.getParamAttributes(ArgIndex);
    if (attrs.hasAttributes()) {
      AttrBuilder B(attrs);
      AttrVec = AttrVec.addParamAttributes(Context, ArgIndex, B);
    }
  }

  // Now add all the implicit arguments
  for (unsigned IID : UsedImplicits) {
    ArgTys.push_back(getIntrinRetType(Context, IID));
    // TODO: Might need to also add any attributes from the intrinsic at some
    // point
  }
  if (!IsCmRT) {
    // Add types of implicit aggregates linearization
    for (const auto &ArgLin : ArgsLin) {
      for (const auto &LinTy : ArgLin.second)
        ArgTys.push_back(LinTy.Ty);
    }
  }

  FunctionType *NFTy = FunctionType::get(F->getReturnType(), ArgTys, false);
  IGC_ASSERT_MESSAGE((NFTy != F->getFunctionType()),
    "type out of sync, expect bool arguments)");

  // Add any function attributes
  AttributeSet FnAttrs = PAL.getFnAttributes();
  if (FnAttrs.hasAttributes()) {
    AttrBuilder B(FnAttrs);
    AttrVec = AttrVec.addAttributes(Context, AttributeList::FunctionIndex, B);
  }

  // Create new function body and insert into the module
  Function *NF = Function::Create(NFTy, F->getLinkage(), F->getName());

  LLVM_DEBUG(dbgs() << "CMImpParam: Transforming From:" << *F);
  vc::transferNameAndCCWithNewAttr(AttrVec, *F, *NF);
  F->getParent()->getFunctionList().insert(F->getIterator(), NF);
  vc::transferDISubprogram(*F, *NF);
  LLVM_DEBUG(dbgs() << "  --> To: " << *NF << "\n");

  // Now to splice the body of the old function into the new function
  NF->getBasicBlockList().splice(NF->begin(), F->getBasicBlockList());

  // Loop over the argument list, transferring uses of the old arguments to the
  // new arguments, also tranferring over the names as well
  std::unordered_map<const Argument *, Argument *> OldToNewArg;
  Function::arg_iterator I2 = NF->arg_begin();
  for (Function::arg_iterator I = F->arg_begin(), E = F->arg_end(); I != E;
       ++I, ++I2) {
    I->replaceAllUsesWith(I2);
    I2->takeName(I);
    OldToNewArg[&*I] = &*I2;
  }

  // Get the insertion point ready for stores to globals
  Instruction &FirstI = *NF->getEntryBlock().begin();
  llvm::SmallVector<uint32_t, 8> ImpKinds;

  for (unsigned IID : UsedImplicits) {
    // We known that for each IID implicit we've already added an arg
    // Rename the arg to something more meaningful here
    IGC_ASSERT_MESSAGE(I2 != NF->arg_end(),
                       "fewer parameters for new function than expected");
    I2->setName(getImplicitArgName(IID));

    if (!isPseudoIntrinsic(IID)) {
      // Also insert a new store at the start of the function to the global
      // variable used for this implicit argument intrinsic
      IGC_ASSERT_MESSAGE(GlobalsMap.count(IID),
                         "no global associated with this imp arg intrinsic");
      new StoreInst(I2, GlobalsMap[IID], &FirstI);
    }

    // Prepare the kinds that will go into the metadata
    ImpKinds.push_back(MapToKind(IID));

    ++I2;
  }

  // Collect arguments linearization to store as metadata.
  genx::ArgToImplicitLinearization LinearizedArgs;
  if (!IsCmRT) {
    for (const auto &ArgLin : ArgsLin) {
      Argument *ExplicitArg = OldToNewArg[ArgLin.first];
      genx::LinearizedArgInfo &LinearizedArg = LinearizedArgs[ExplicitArg];
      for (const auto &LinTy : ArgLin.second) {
        I2->setName("__arg_lin_" + ExplicitArg->getName() + "." +
                    std::to_string(LinTy.Offset));
        ImpKinds.push_back(genx::KernelMetadata::AK_NORMAL |
                           genx::KernelMetadata::IMP_OCL_LINEARIZATION);
        auto &Ctx = F->getContext();
        auto *I32Ty = Type::getInt32Ty(Ctx);
        ConstantInt *Offset = ConstantInt::get(I32Ty, LinTy.Offset);
        LinearizedArg.push_back({&*I2, Offset});
        ++I2;
      }
    }
  }

  CallGraph &CG = getAnalysis<CallGraphWrapperPass>().getCallGraph();
  CallGraphNode *NF_CGN = CG.getOrInsertFunction(NF);

  if (F->hasDLLExportStorageClass())
    NF->setDLLStorageClass(F->getDLLStorageClass());

  genx::replaceFunctionRefMD(*F, *NF);

  SmallVector<unsigned, 8> ArgKinds;
  genx::KernelMetadata KM(NF);
  // Update arg kinds for the NF.
  for (unsigned i = 0; i < KM.getNumArgs(); ++i) {
    if (LinearizedArgs.count(IGCLLVM::getArg(*NF, i)))
      ArgKinds.push_back(genx::KernelMetadata::AK_NORMAL |
                         genx::KernelMetadata::IMP_OCL_BYVALSVM);
    else
      ArgKinds.push_back(KM.getArgKind(i));
  }
  std::copy(ImpKinds.begin(), ImpKinds.end(), std::back_inserter(ArgKinds));
  KM.updateArgKindsMD(std::move(ArgKinds));
  KM.updateLinearizationMD(std::move(LinearizedArgs));

  // Now that the old function is dead, delete it. If there is a dangling
  // reference to the CallGraphNode, just leave the dead function around
  NF_CGN->stealCalledFunctionsFrom(CG[F]);
  CallGraphNode *CGN = CG[F];
  if (CGN->getNumReferences() == 0)
    delete CG.removeFunctionFromModule(CGN);
  else
    F->setLinkage(Function::ExternalLinkage);

  return NF_CGN;
}

char CMImpParam::ID = 0;
INITIALIZE_PASS_BEGIN(CMImpParam, "cmimpparam",
                      "Transformations required to support implicit arguments",
                      false, false)
INITIALIZE_PASS_DEPENDENCY(CallGraphWrapperPass)
INITIALIZE_PASS_END(CMImpParam, "cmimpparam",
                    "Transformations required to support implicit arguments",
                    false, false)

Pass *llvm::createCMImpParamPass(bool IsCMRT) { return new CMImpParam(IsCMRT); }
